{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCM Extractor Tutorial: Working with the Codebase\n",
    "\n",
    "This notebook provides a comprehensive tutorial for working with the FCM Extractor codebase - a tool for extracting Fuzzy Cognitive Maps (FCMs) from interview transcripts using NLP, clustering, and semantic analysis.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Project Overview](#project-overview)\n",
    "2. [Setting Up the Environment](#setting-up-environment)\n",
    "3. [Understanding the Codebase Structure](#codebase-structure)\n",
    "4. [Basic Usage Examples](#basic-usage)\n",
    "5. [Advanced Configuration](#configuration)\n",
    "6. [Pipeline Components](#pipeline-components)\n",
    "7. [Working with Results](#results)\n",
    "8. [Common Tasks and Examples](#examples)\n",
    "9. [Troubleshooting](#troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Overview {#project-overview}\n",
    "\n",
    "The FCM Extractor is a Python package that automates the extraction of Fuzzy Cognitive Maps from qualitative interview data. It uses:\n",
    "\n",
    "- **NLP and LLMs** for concept extraction\n",
    "- **Semantic clustering** using embeddings and LLMs \n",
    "- **Edge inference** to identify causal relationships\n",
    "- **Interactive visualization** for exploring results\n",
    "\n",
    "### Key Features:\n",
    "- Automated concept extraction from interview transcripts\n",
    "- Advanced clustering with hybrid embedding/LLM approaches\n",
    "- Causal relationship inference with confidence scoring\n",
    "- Interactive HTML visualizations\n",
    "- Evaluation against ground truth FCMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment {#setting-up-environment}\n",
    "\n",
    "### Prerequisites and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure you're in the correct directory\n",
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Change to fcm_extractor directory if needed\n",
    "if not os.path.exists('fcm_extractor'):\n",
    "    print(\"Please navigate to the directory containing 'fcm_extractor' folder\")\n",
    "else:\n",
    "    os.chdir('fcm_extractor')\n",
    "    print(\"Changed to:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API keys (replace with your actual keys)\n",
    "import os\n",
    "\n",
    "# Set your API keys here - NEVER commit these to git!\n",
    "os.environ['OPENAI_API_KEY'] = 'your-openai-api-key-here'\n",
    "os.environ['GOOGLE_API_KEY'] = 'your-google-api-key-here'\n",
    "\n",
    "# Verify API keys are set\n",
    "print(\"OpenAI API key set:\", bool(os.environ.get('OPENAI_API_KEY')))\n",
    "print(\"Google API key set:\", bool(os.environ.get('GOOGLE_API_KEY')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding the Codebase Structure {#codebase-structure}\n",
    "\n",
    "Let's explore the project structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the project structure\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def show_tree(path, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    \"\"\"Display directory tree structure\"\"\"\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return\n",
    "    \n",
    "    items = sorted([p for p in path.iterdir() if not p.name.startswith('.')],\n",
    "                  key=lambda x: (x.is_file(), x.name.lower()))\n",
    "    \n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        current_prefix = \"└── \" if is_last else \"├── \"\n",
    "        print(f\"{prefix}{current_prefix}{item.name}\")\n",
    "        \n",
    "        if item.is_dir() and current_depth < max_depth:\n",
    "            next_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "            show_tree(item, next_prefix, max_depth, current_depth + 1)\n",
    "\n",
    "print(\"FCM Extractor Project Structure:\")\n",
    "show_tree(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Components Explanation:\n",
    "\n",
    "- **`run_extraction.py`**: Main entry point for running FCM extraction\n",
    "- **`config/constants.py`**: All configuration parameters\n",
    "- **`src/core/`**: Core concept extraction and graph building\n",
    "- **`src/clustering/`**: Concept clustering algorithms\n",
    "- **`src/edge_inference/`**: Causal relationship inference\n",
    "- **`src/pipeline/`**: Complete processing pipeline\n",
    "- **`utils/`**: Visualization, scoring, and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Usage Examples {#basic-usage}\n",
    "\n",
    "### Running the Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic usage: Process the default interview file\n",
    "# This is equivalent to running: python run_extraction.py\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from src.pipeline import process_interviews\n",
    "from config.constants import INTERVIEWS_DIRECTORY, OUTPUT_DIRECTORY, DEFAULT_INTERVIEW_FILE\n",
    "\n",
    "print(f\"Default interview file: {DEFAULT_INTERVIEW_FILE}\")\n",
    "print(f\"Input directory: {INTERVIEWS_DIRECTORY}\")\n",
    "print(f\"Output directory: {OUTPUT_DIRECTORY}\")\n",
    "\n",
    "# Process the default file (uncomment to run)\n",
    "# results = process_interviews(\n",
    "#     interviews_dir=INTERVIEWS_DIRECTORY,\n",
    "#     output_dir=OUTPUT_DIRECTORY,\n",
    "#     specific_file=DEFAULT_INTERVIEW_FILE\n",
    "# )\n",
    "# print(f\"Processed {len(results)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a specific interview file\n",
    "specific_file = \"BD007.docx\"  # Change this to any interview file you want\n",
    "\n",
    "# Uncomment to run processing:\n",
    "# results = process_interviews(\n",
    "#     interviews_dir=INTERVIEWS_DIRECTORY,\n",
    "#     output_dir=OUTPUT_DIRECTORY, \n",
    "#     specific_file=specific_file\n",
    "# )\n",
    "\n",
    "print(f\"To process {specific_file}, uncomment the lines above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine what happens in the pipeline step by step\n",
    "from src.core.extract_concepts import extract_concepts_with_metadata\n",
    "from src.clustering.improved_clustering import cluster_concepts_with_metadata\n",
    "from src.edge_inference.edge_inference import infer_edges\n",
    "\n",
    "# Sample interview text (normally this would come from a .docx file)\n",
    "sample_text = \"\"\"\n",
    "I find that when I'm stressed at work, it really affects my sleep quality. \n",
    "Poor sleep then leads to reduced concentration the next day, which impacts my productivity. \n",
    "This creates a cycle where I feel more stressed because I'm not getting things done efficiently.\n",
    "Exercise helps break this cycle - when I work out regularly, I sleep better and feel less stressed.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample interview text:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Configuration {#configuration}\n",
    "\n",
    "The system is highly configurable through `config/constants.py`. Let's explore key settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View current configuration settings\n",
    "from config import constants\n",
    "\n",
    "print(\"=== MODEL CONFIGURATION ===\")\n",
    "print(f\"Concept Extraction Model: {constants.CONCEPT_EXTRACTION_MODEL}\")\n",
    "print(f\"Edge Inference Model: {constants.EDGE_INFERENCE_MODEL}\")\n",
    "print(f\"LLM Clustering Model: {constants.LLM_CLUSTERING_MODEL}\")\n",
    "\n",
    "print(\"\\n=== CLUSTERING CONFIGURATION ===\")\n",
    "print(f\"Clustering Method: {constants.CLUSTERING_METHOD}\")\n",
    "print(f\"Embedding Model: {constants.CLUSTERING_EMBEDDING_MODEL}\")\n",
    "print(f\"Algorithm: {constants.CLUSTERING_ALGORITHM}\")\n",
    "print(f\"Min Cluster Size: {constants.HDBSCAN_MIN_CLUSTER_SIZE}\")\n",
    "\n",
    "print(\"\\n=== EDGE INFERENCE CONFIGURATION ===\")\n",
    "print(f\"Confidence Threshold: {constants.EDGE_CONFIDENCE_THRESHOLD}\")\n",
    "print(f\"Use Confidence Filtering: {constants.USE_CONFIDENCE_FILTERING}\")\n",
    "print(f\"Enable Intra-cluster Edges: {constants.ENABLE_INTRA_CLUSTER_EDGES}\")\n",
    "\n",
    "print(\"\\n=== POST-CLUSTERING CONFIGURATION ===\")\n",
    "print(f\"Enable Post-clustering: {constants.ENABLE_POST_CLUSTERING}\")\n",
    "print(f\"Similarity Threshold: {constants.POST_CLUSTERING_SIMILARITY_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to modify configuration at runtime\n",
    "import config.constants as config\n",
    "\n",
    "# Example: Change clustering method\n",
    "print(f\"Current clustering method: {config.CLUSTERING_METHOD}\")\n",
    "\n",
    "# Temporarily change it (this affects the current session only)\n",
    "original_method = config.CLUSTERING_METHOD\n",
    "config.CLUSTERING_METHOD = \"embedding_enhanced\"  # Options: llm_only, hybrid, embedding_enhanced\n",
    "print(f\"Changed to: {config.CLUSTERING_METHOD}\")\n",
    "\n",
    "# Reset it back\n",
    "config.CLUSTERING_METHOD = original_method\n",
    "print(f\"Reset to: {config.CLUSTERING_METHOD}\")\n",
    "\n",
    "print(\"\\nNote: To make permanent changes, edit config/constants.py directly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pipeline Components {#pipeline-components}\n",
    "\n",
    "Let's examine each component of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Concept Extraction\n",
    "from src.core.extract_concepts import extract_concepts_with_metadata\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Work stress significantly impacts my ability to maintain work-life balance.\n",
    "When deadlines are tight, I tend to skip exercise and social activities.\n",
    "This leads to increased anxiety and reduced job satisfaction over time.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== CONCEPT EXTRACTION ===\")\n",
    "print(f\"Input text: {sample_text.strip()}\")\n",
    "print(\"\\nExtracting concepts...\")\n",
    "\n",
    "# This would normally call the LLM - we'll show what the output looks like\n",
    "# concepts_with_meta = extract_concepts_with_metadata(sample_text)\n",
    "\n",
    "# Mock output for demonstration\n",
    "mock_concepts = [\n",
    "    \"work stress\", \"work-life balance\", \"deadlines\", \"exercise\", \n",
    "    \"social activities\", \"anxiety\", \"job satisfaction\"\n",
    "]\n",
    "\n",
    "print(f\"Extracted concepts: {mock_concepts}\")\n",
    "print(f\"Number of concepts: {len(mock_concepts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Concept Clustering\n",
    "from src.clustering.improved_clustering import cluster_concepts_with_metadata\n",
    "\n",
    "print(\"=== CONCEPT CLUSTERING ===\")\n",
    "print(\"Grouping related concepts...\")\n",
    "\n",
    "# Mock clustering result\n",
    "mock_clusters = {\n",
    "    \"Work Stressors\": [\"work stress\", \"deadlines\"],\n",
    "    \"Wellness Activities\": [\"exercise\", \"social activities\"],\n",
    "    \"Psychological Outcomes\": [\"anxiety\", \"job satisfaction\"],\n",
    "    \"Life Balance\": [\"work-life balance\"]\n",
    "}\n",
    "\n",
    "print(\"\\nClustering results:\")\n",
    "for cluster_name, concepts in mock_clusters.items():\n",
    "    print(f\"  {cluster_name}: {concepts}\")\n",
    "\n",
    "print(f\"\\nNumber of clusters: {len(mock_clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Edge Inference\n",
    "print(\"=== EDGE INFERENCE ===\")\n",
    "print(\"Identifying causal relationships between clusters...\")\n",
    "\n",
    "# Mock edge inference results\n",
    "mock_edges = [\n",
    "    {\"source\": \"Work Stressors\", \"target\": \"Wellness Activities\", \n",
    "     \"relationship\": \"negative\", \"confidence\": 0.85, \"weight\": -0.7},\n",
    "    {\"source\": \"Work Stressors\", \"target\": \"Psychological Outcomes\", \n",
    "     \"relationship\": \"negative\", \"confidence\": 0.90, \"weight\": -0.8},\n",
    "    {\"source\": \"Wellness Activities\", \"target\": \"Psychological Outcomes\", \n",
    "     \"relationship\": \"positive\", \"confidence\": 0.75, \"weight\": 0.6},\n",
    "    {\"source\": \"Work Stressors\", \"target\": \"Life Balance\", \n",
    "     \"relationship\": \"negative\", \"confidence\": 0.88, \"weight\": -0.75}\n",
    "]\n",
    "\n",
    "print(\"\\nInferred relationships:\")\n",
    "for edge in mock_edges:\n",
    "    print(f\"  {edge['source']} → {edge['target']} ({edge['relationship']}, conf: {edge['confidence']})\")\n",
    "\n",
    "print(f\"\\nNumber of edges: {len(mock_edges)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Working with Results {#results}\n",
    "\n",
    "Let's explore how to work with the output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine typical output structure\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Look for existing outputs\n",
    "output_dir = Path(\"../fcm_outputs_gpt-mini\")\n",
    "if output_dir.exists():\n",
    "    print(\"Available output directories:\")\n",
    "    for subdir in output_dir.iterdir():\n",
    "        if subdir.is_dir():\n",
    "            files = list(subdir.glob(\"*\"))\n",
    "            print(f\"  {subdir.name}: {len(files)} files\")\n",
    "            \n",
    "    # Look at a specific output\n",
    "    sample_dirs = [d for d in output_dir.iterdir() if d.is_dir()]\n",
    "    if sample_dirs:\n",
    "        sample_dir = sample_dirs[0]\n",
    "        print(f\"\\nFiles in {sample_dir.name}:\")\n",
    "        for file in sample_dir.iterdir():\n",
    "            print(f\"  - {file.name}\")\nelse:\n",
    "    print(\"No output directory found. Run the pipeline first to generate results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine an FCM JSON file\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to load a sample FCM file\n",
    "output_dir = Path(\"../fcm_outputs_gpt-mini\")\n",
    "fcm_files = list(output_dir.glob(\"*/*/fcm.json\"))\n",
    "\n",
    "if fcm_files:\n",
    "    sample_fcm_file = fcm_files[0]\n",
    "    print(f\"Loading FCM from: {sample_fcm_file}\")\n",
    "    \n",
    "    with open(sample_fcm_file, 'r') as f:\n",
    "        fcm_data = json.load(f)\n",
    "    \n",
    "    print(\"\\n=== FCM DATA STRUCTURE ===\")\n",
    "    print(f\"Keys in FCM data: {list(fcm_data.keys())}\")\n",
    "    \n",
    "    if 'nodes' in fcm_data:\n",
    "        print(f\"\\nNumber of nodes: {len(fcm_data['nodes'])}\")\n",
    "        print(\"First few nodes:\")\n",
    "        for i, node in enumerate(fcm_data['nodes'][:3]):\n",
    "            print(f\"  {i+1}. {node}\")\n",
    "    \n",
    "    if 'edges' in fcm_data:\n",
    "        print(f\"\\nNumber of edges: {len(fcm_data['edges'])}\")\n",
    "        print(\"First few edges:\")\n",
    "        for i, edge in enumerate(fcm_data['edges'][:3]):\n",
    "            print(f\"  {i+1}. {edge['source']} → {edge['target']} (weight: {edge.get('weight', 'N/A')})\")\nelse:\n",
    "    print(\"No FCM files found. Generate some results first!\")\n",
    "    \n",
    "    # Show what an FCM structure typically looks like\n",
    "    sample_fcm = {\n",
    "        \"nodes\": [\n",
    "            {\"id\": \"cluster_1\", \"label\": \"Work Stressors\", \"type\": \"cluster\"},\n",
    "            {\"id\": \"cluster_2\", \"label\": \"Wellness Activities\", \"type\": \"cluster\"}\n",
    "        ],\n",
    "        \"edges\": [\n",
    "            {\n",
    "                \"source\": \"cluster_1\",\n",
    "                \"target\": \"cluster_2\", \n",
    "                \"weight\": -0.7,\n",
    "                \"confidence\": 0.85,\n",
    "                \"relationship_type\": \"negative\"\n",
    "            }\n",
    "        ],\n",
    "        \"metadata\": {\n",
    "            \"document\": \"sample_interview\",\n",
    "            \"extraction_date\": \"2024-01-01\",\n",
    "            \"num_concepts_extracted\": 15,\n",
    "            \"num_clusters\": 4\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== SAMPLE FCM STRUCTURE ===\")\n",
    "    print(json.dumps(sample_fcm, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Common Tasks and Examples {#examples}\n",
    "\n",
    "### Task 1: Scoring FCMs Against Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring FCMs against ground truth\n",
    "from utils.score_fcm import score_fcm_semantic\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== FCM SCORING ===\")\n",
    "\n",
    "# Show available ground truth files\n",
    "gt_dir = Path(\"../ground_truth\")\n",
    "if gt_dir.exists():\n",
    "    gt_files = list(gt_dir.glob(\"*.csv\"))\n",
    "    print(f\"Available ground truth files: {len(gt_files)}\")\n",
    "    for gt_file in gt_files[:5]:  # Show first 5\n",
    "        print(f\"  - {gt_file.name}\")\n",
    "        \n",
    "    # Example scoring command (uncomment to run)\n",
    "    if gt_files:\n",
    "        sample_gt = gt_files[0]\n",
    "        print(f\"\\nTo score against {sample_gt.name}:\")\n",
    "        print(f\"python utils/score_fcm.py --gt-path {sample_gt} --gen-path ../fcm_outputs/sample/sample_fcm.json\")\nelse:\n",
    "    print(\"Ground truth directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding scoring metrics\n",
    "print(\"=== SCORING METRICS EXPLAINED ===\")\n",
    "\n",
    "scoring_info = \"\"\"\n",
    "The scoring system uses semantic similarity to compare generated FCMs with ground truth:\n",
    "\n",
    "1. **Node Matching**: \n",
    "   - Uses embeddings to find semantically similar concepts\n",
    "   - Threshold-based matching (default: 0.7 similarity)\n",
    "\n",
    "2. **Edge Matching**:\n",
    "   - Compares causal relationships between matched nodes\n",
    "   - Considers relationship direction and polarity\n",
    "\n",
    "3. **Metrics Computed**:\n",
    "   - Precision: % of generated edges that match ground truth\n",
    "   - Recall: % of ground truth edges found in generated FCM\n",
    "   - F1-Score: Harmonic mean of precision and recall\n",
    "   - Semantic similarity scores for nodes and edges\n",
    "\n",
    "4. **Output Files**:\n",
    "   - `*_scoring_results.csv`: Detailed matching results\n",
    "   - `*_generated_matrix.csv`: Generated adjacency matrix\n",
    "\"\"\"\n",
    "\n",
    "print(scoring_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Creating Custom Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating visualizations\n",
    "from utils.visualize_fcm import create_interactive_visualization\n",
    "import json\n",
    "\n",
    "print(\"=== FCM VISUALIZATION ===\")\n",
    "\n",
    "# Create a sample FCM for visualization\n",
    "sample_fcm_data = {\n",
    "    \"nodes\": [\n",
    "        {\"id\": \"stress\", \"label\": \"Work Stress\", \"type\": \"cluster\", \"concepts\": [\"deadlines\", \"workload\"]},\n",
    "        {\"id\": \"wellness\", \"label\": \"Wellness\", \"type\": \"cluster\", \"concepts\": [\"exercise\", \"sleep\"]},\n",
    "        {\"id\": \"performance\", \"label\": \"Performance\", \"type\": \"cluster\", \"concepts\": [\"productivity\", \"focus\"]}\n",
    "    ],\n",
    "    \"edges\": [\n",
    "        {\"source\": \"stress\", \"target\": \"wellness\", \"weight\": -0.7, \"confidence\": 0.85},\n",
    "        {\"source\": \"wellness\", \"target\": \"performance\", \"weight\": 0.6, \"confidence\": 0.78},\n",
    "        {\"source\": \"stress\", \"target\": \"performance\", \"weight\": -0.5, \"confidence\": 0.82}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Sample FCM structure:\")\n",
    "print(json.dumps(sample_fcm_data, indent=2))\n",
    "\n",
    "# Create visualization (uncomment to run)\n",
    "# output_file = \"sample_fcm_interactive.html\"\n",
    "# create_interactive_visualization(sample_fcm_data, output_file)\n",
    "# print(f\"\\nVisualization created: {output_file}\")\n",
    "# print(\"Open this file in a web browser to view the interactive FCM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Batch Processing Multiple Interviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing multiple interviews\n",
    "from src.pipeline import process_interviews\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== BATCH PROCESSING ===\")\n",
    "\n",
    "# Check available interview files\n",
    "interviews_dir = Path(\"../interviews\")\n",
    "if interviews_dir.exists():\n",
    "    interview_files = list(interviews_dir.glob(\"*.docx\")) + list(interviews_dir.glob(\"*.doc\"))\n",
    "    print(f\"Found {len(interview_files)} interview files:\")\n",
    "    for file in interview_files[:10]:  # Show first 10\n",
    "        print(f\"  - {file.name}\")\n",
    "    \n",
    "    # Example batch processing (uncomment to run)\n",
    "    print(\"\\nTo process all files:\")\n",
    "    print(\"python run_extraction.py --all\")\n",
    "    \n",
    "    # Or programmatically:\n",
    "    # results = process_interviews(\n",
    "    #     interviews_dir=\"../interviews\",\n",
    "    #     output_dir=\"../fcm_outputs\",\n",
    "    #     specific_file=None  # Process all files\n",
    "    # )\nelse:\n",
    "    print(\"Interviews directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Custom Configuration for Different Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizing configuration for different domains\n",
    "import config.constants as config\n",
    "\n",
    "print(\"=== DOMAIN-SPECIFIC CONFIGURATIONS ===\")\n",
    "\n",
    "# Configuration for medical/healthcare interviews\n",
    "healthcare_config = {\n",
    "    \"CLUSTERING_EMBEDDING_MODEL\": \"sentence-transformers/allenai-specter\",  # Good for scientific text\n",
    "    \"HDBSCAN_MIN_CLUSTER_SIZE\": 2,  # Smaller clusters for detailed medical concepts\n",
    "    \"EDGE_CONFIDENCE_THRESHOLD\": 0.8,  # Higher confidence for medical relationships\n",
    "    \"POST_CLUSTERING_SIMILARITY_THRESHOLD\": 0.7  # Conservative merging\n",
    "}\n",
    "\n",
    "# Configuration for general social science interviews\n",
    "social_science_config = {\n",
    "    \"CLUSTERING_EMBEDDING_MODEL\": \"sentence-transformers/all-mpnet-base-v2\",  # Good general model\n",
    "    \"HDBSCAN_MIN_CLUSTER_SIZE\": 3,  # Moderate clustering\n",
    "    \"EDGE_CONFIDENCE_THRESHOLD\": 0.7,  # Standard confidence\n",
    "    \"POST_CLUSTERING_SIMILARITY_THRESHOLD\": 0.6  # Moderate merging\n",
    "}\n",
    "\n",
    "# Configuration for business/organizational interviews\n",
    "business_config = {\n",
    "    \"CLUSTERING_EMBEDDING_MODEL\": \"sentence-transformers/all-MiniLM-L12-v2\",  # Fast processing\n",
    "    \"HDBSCAN_MIN_CLUSTER_SIZE\": 4,  # Larger clusters for high-level concepts\n",
    "    \"EDGE_CONFIDENCE_THRESHOLD\": 0.6,  # Lower threshold for exploratory analysis\n",
    "    \"POST_CLUSTERING_SIMILARITY_THRESHOLD\": 0.5  # Aggressive merging\n",
    "}\n",
    "\n",
    "configs = {\n",
    "    \"Healthcare/Medical\": healthcare_config,\n",
    "    \"Social Science\": social_science_config, \n",
    "    \"Business/Organizational\": business_config\n",
    "}\n",
    "\n",
    "for domain, cfg in configs.items():\n",
    "    print(f\"\\n{domain} Configuration:\")\n",
    "    for key, value in cfg.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nTo apply a configuration, modify config/constants.py with these values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Troubleshooting {#troubleshooting}\n",
    "\n",
    "Common issues and solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging and troubleshooting tools\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== SYSTEM CHECK ===\")\n",
    "\n",
    "# Check Python version\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if we can import key modules\n",
    "modules_to_check = [\n",
    "    'openai', 'sentence_transformers', 'sklearn', 'networkx', \n",
    "    'pandas', 'numpy', 'matplotlib', 'plotly'\n",
    "]\n",
    "\n",
    "print(\"\\nModule availability:\")\n",
    "for module in modules_to_check:\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"  ✅ {module}\")\n",
    "    except ImportError:\n",
    "        print(f\"  ❌ {module} - run: pip install {module}\")\n",
    "\n",
    "# Check API keys\n",
    "print(\"\\nAPI Keys:\")\n",
    "print(f\"  OpenAI: {'✅' if os.environ.get('OPENAI_API_KEY') else '❌ Not set'}\")\n",
    "print(f\"  Google: {'✅' if os.environ.get('GOOGLE_API_KEY') else '❌ Not set'}\")\n",
    "\n",
    "# Check directory structure\n",
    "print(\"\\nDirectory Structure:\")\n",
    "required_dirs = ['config', 'src', 'utils']\n",
    "for dir_name in required_dirs:\n",
    "    exists = Path(dir_name).exists()\n",
    "    print(f\"  {dir_name}: {'✅' if exists else '❌ Missing'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common troubleshooting tips\n",
    "troubleshooting_guide = \"\"\"\n",
    "=== COMMON ISSUES AND SOLUTIONS ===\n",
    "\n",
    "1. **\"Module not found\" errors**:\n",
    "   - Run: pip install -r requirements.txt\n",
    "   - Make sure you're in the fcm_extractor directory\n",
    "\n",
    "2. **API key errors**:\n",
    "   - Set environment variables: export OPENAI_API_KEY=\"your-key\"\n",
    "   - Check that your API keys are valid and have sufficient credits\n",
    "\n",
    "3. **\"No documents processed\" error**:\n",
    "   - Check that interview files exist in the specified directory\n",
    "   - Verify file formats (.docx, .doc, .txt are supported)\n",
    "   - Check file permissions\n",
    "\n",
    "4. **Memory errors during processing**:\n",
    "   - Reduce HDBSCAN_MIN_CLUSTER_SIZE for smaller clusters\n",
    "   - Use faster embedding models (e.g., all-MiniLM-L6-v2)\n",
    "   - Process files individually instead of batch processing\n",
    "\n",
    "5. **Poor clustering results**:\n",
    "   - Adjust HDBSCAN_MIN_CLUSTER_SIZE (lower = more clusters)\n",
    "   - Try different embedding models\n",
    "   - Enable/disable post-clustering merge\n",
    "\n",
    "6. **Few or no edges in FCM**:\n",
    "   - Lower EDGE_CONFIDENCE_THRESHOLD (e.g., from 0.7 to 0.5)\n",
    "   - Enable ENABLE_INTRA_CLUSTER_EDGES for more connections\n",
    "   - Check that your interview text contains causal language\n",
    "\n",
    "7. **Visualization not displaying**:\n",
    "   - Open HTML files in a modern web browser\n",
    "   - Check that the FCM JSON file has nodes and edges\n",
    "   - Try the static PNG visualization instead\n",
    "\n",
    "8. **Logging issues**:\n",
    "   - Set ENABLE_FILE_LOGGING = True in constants.py\n",
    "   - Check that the logs directory has write permissions\n",
    "   - Look in the logs/ directory for detailed error messages\n",
    "\"\"\"\n",
    "\n",
    "print(troubleshooting_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial covered the essential aspects of working with the FCM Extractor codebase:\n",
    "\n",
    "1. **Project structure** and key components\n",
    "2. **Basic usage** for processing interview transcripts\n",
    "3. **Advanced configuration** for different domains and use cases\n",
    "4. **Pipeline components** (concept extraction, clustering, edge inference)\n",
    "5. **Working with results** (FCM files, visualizations, scoring)\n",
    "6. **Common tasks** and practical examples\n",
    "7. **Troubleshooting** common issues\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Practice with your data**: Try processing your own interview files\n",
    "2. **Experiment with configurations**: Test different clustering and inference settings\n",
    "3. **Evaluate results**: Use the scoring system to validate against ground truth\n",
    "4. **Customize for your domain**: Adapt the configuration for your specific research area\n",
    "5. **Extend the pipeline**: Add custom processing steps as needed\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- `README.md`: Comprehensive documentation\n",
    "- `config/constants.py`: All configuration options\n",
    "- `utils/`: Utility scripts for scoring, visualization, etc.\n",
    "- `logs/`: Processing logs for debugging\n",
    "\n",
    "Happy FCM extraction! 🗺️📊"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}